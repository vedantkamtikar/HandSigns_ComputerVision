{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0811e32c-40c4-47a3-9c55-eab392c7f5a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder, ImageNet\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "import random, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c9f2fd-503e-4a57-bc11-463f54edd43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch can access the GPU.\n",
      "Number of GPUs available: 1\n",
      "Current GPU Name: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch can access the GPU.\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is using the CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ad652a-a711-491c-b253-d2d5c2d32f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(\n",
    "    root = \"newdata\",\n",
    "    transform = transform\n",
    "\n",
    ")\n",
    "#'dataset' is used to identify the folder in which we have our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1080c3f5-4ee5-430f-af8a-690669587bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "Number of classes: 36\n"
     ]
    }
   ],
   "source": [
    "class_names = dataset.classes\n",
    "num_classes = len(class_names)\n",
    "print(class_names)\n",
    "print(f\"Number of classes: {num_classes}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91169c6d-5408-480e-b061-08d11d8d4d3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 20 train, 5 val\n",
      "1: 20 train, 5 val\n",
      "2: 20 train, 5 val\n",
      "3: 20 train, 5 val\n",
      "4: 20 train, 5 val\n",
      "5: 20 train, 5 val\n",
      "6: 20 train, 5 val\n",
      "7: 20 train, 5 val\n",
      "8: 20 train, 5 val\n",
      "9: 20 train, 5 val\n",
      "A: 20 train, 5 val\n",
      "B: 20 train, 5 val\n",
      "C: 20 train, 5 val\n",
      "D: 20 train, 5 val\n",
      "E: 20 train, 5 val\n",
      "F: 20 train, 5 val\n",
      "G: 20 train, 5 val\n",
      "H: 20 train, 5 val\n",
      "I: 20 train, 5 val\n",
      "J: 20 train, 5 val\n",
      "K: 20 train, 5 val\n",
      "L: 20 train, 5 val\n",
      "M: 20 train, 5 val\n",
      "N: 20 train, 5 val\n",
      "O: 20 train, 5 val\n",
      "P: 20 train, 5 val\n",
      "Q: 20 train, 5 val\n",
      "R: 20 train, 5 val\n",
      "S: 20 train, 5 val\n",
      "T: 20 train, 5 val\n",
      "U: 20 train, 5 val\n",
      "V: 20 train, 5 val\n",
      "W: 20 train, 5 val\n",
      "X: 20 train, 5 val\n",
      "Y: 20 train, 5 val\n",
      "Z: 20 train, 5 val\n",
      "\n",
      "Train / Validation split completed.\n"
     ]
    }
   ],
   "source": [
    "SOURCE_DIR = \"newdata\"\n",
    "OUTPUT_DIR = \"data\"\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, split), exist_ok=True)\n",
    "\n",
    "for class_name in os.listdir(SOURCE_DIR):\n",
    "    class_path = os.path.join(SOURCE_DIR, class_name)\n",
    "\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    images = [\n",
    "        img for img in os.listdir(class_path)\n",
    "        if img.lower().endswith((\".jpg\", \".png\", \".jpeg\"))\n",
    "    ]\n",
    "\n",
    "    random.shuffle(images)\n",
    "\n",
    "    n_train = int(len(images) * TRAIN_RATIO)\n",
    "    train_imgs = images[:n_train]\n",
    "    val_imgs = images[n_train:]\n",
    "\n",
    "    for split, split_imgs in [(\"train\", train_imgs), (\"val\", val_imgs)]:\n",
    "        split_class_dir = os.path.join(OUTPUT_DIR, split, class_name)\n",
    "        os.makedirs(split_class_dir, exist_ok=True)\n",
    "\n",
    "        for img in split_imgs:\n",
    "            shutil.copy(\n",
    "                os.path.join(class_path, img),\n",
    "                os.path.join(split_class_dir, img)\n",
    "            )\n",
    "\n",
    "    print(f\"{class_name}: {len(train_imgs)} train, {len(val_imgs)} val\")\n",
    "\n",
    "print(\"\\nTrain / Validation split completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bad2af02-21ab-4de4-8ad3-7974bc294a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(\n",
    "    root=\"data/train\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = ImageFolder(\n",
    "    root=\"data/val\",\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41749213-dc7e-4b47-974a-3dcb2abfff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c527a5e-a39b-47fd-9508-b1dd265291d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "weights = MobileNet_V2_Weights.DEFAULT\n",
    "model = mobilenet_v2(weights=weights)\n",
    "\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "#freezing the feature extractor\n",
    "\n",
    "model.classifier[1] = nn.Linear(\n",
    "    model.last_channel,\n",
    "    num_classes \n",
    "    #Linear(1280 → 36)\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b05950ad-cd85-443e-987c-6daf3f8578d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.classifier.parameters(),\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a4541c8-d3eb-40f6-bcb7-8e43a988e165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Train Loss: 3.5624, Train Acc: 3.61% - Val Loss: 3.4844, Val Acc: 8.89%\n",
      "Epoch [2/20] - Train Loss: 3.4300, Train Acc: 12.50% - Val Loss: 3.3770, Val Acc: 15.56%\n",
      "Epoch [3/20] - Train Loss: 3.3228, Train Acc: 24.72% - Val Loss: 3.2800, Val Acc: 27.22%\n",
      "Epoch [4/20] - Train Loss: 3.2211, Train Acc: 34.17% - Val Loss: 3.1907, Val Acc: 43.33%\n",
      "Epoch [5/20] - Train Loss: 3.1260, Train Acc: 45.56% - Val Loss: 3.0970, Val Acc: 50.56%\n",
      "Epoch [6/20] - Train Loss: 3.0325, Train Acc: 53.89% - Val Loss: 3.0185, Val Acc: 50.56%\n",
      "Epoch [7/20] - Train Loss: 2.9520, Train Acc: 61.39% - Val Loss: 2.9323, Val Acc: 60.00%\n",
      "Epoch [8/20] - Train Loss: 2.8393, Train Acc: 69.17% - Val Loss: 2.8502, Val Acc: 65.00%\n",
      "Epoch [9/20] - Train Loss: 2.7552, Train Acc: 74.58% - Val Loss: 2.7945, Val Acc: 63.33%\n",
      "Epoch [10/20] - Train Loss: 2.6750, Train Acc: 75.00% - Val Loss: 2.7009, Val Acc: 70.00%\n",
      "Epoch [11/20] - Train Loss: 2.5935, Train Acc: 76.67% - Val Loss: 2.6307, Val Acc: 68.89%\n",
      "Epoch [12/20] - Train Loss: 2.5196, Train Acc: 79.03% - Val Loss: 2.5699, Val Acc: 71.11%\n",
      "Epoch [13/20] - Train Loss: 2.4261, Train Acc: 79.44% - Val Loss: 2.5050, Val Acc: 71.67%\n",
      "Epoch [14/20] - Train Loss: 2.3711, Train Acc: 81.11% - Val Loss: 2.4350, Val Acc: 72.22%\n",
      "Epoch [15/20] - Train Loss: 2.2975, Train Acc: 83.75% - Val Loss: 2.3671, Val Acc: 78.33%\n",
      "Epoch [16/20] - Train Loss: 2.2299, Train Acc: 83.75% - Val Loss: 2.3216, Val Acc: 75.00%\n",
      "Epoch [17/20] - Train Loss: 2.1786, Train Acc: 85.00% - Val Loss: 2.2887, Val Acc: 73.33%\n",
      "Epoch [18/20] - Train Loss: 2.1203, Train Acc: 84.17% - Val Loss: 2.2136, Val Acc: 75.00%\n",
      "Epoch [19/20] - Train Loss: 2.0467, Train Acc: 87.08% - Val Loss: 2.1843, Val Acc: 75.00%\n",
      "Epoch [20/20] - Train Loss: 1.9673, Train Acc: 85.97% - Val Loss: 2.1329, Val Acc: 76.11%\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #train\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    #val\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "        f\"- Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}% \"\n",
    "        f\"- Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "884df1f2-fe1b-491b-b99c-e567ca0b7fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"mobilenet_gesture.pth\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f1ac31d-9284-43f3-aba6-9f806bac1d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"class_names.json\", \"w\") as f:\n",
    "    json.dump(class_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d0480ad-f84e-4936-b015-555c20543044",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "weights = MobileNet_V2_Weights.DEFAULT\n",
    "model = mobilenet_v2(weights=None)  # no pretrained at inference\n",
    "\n",
    "model.classifier[1] = torch.nn.Linear(\n",
    "    model.last_channel,\n",
    "    len(class_names)\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"mobilenet_gesture.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad2bf30d-dce2-4541-bc38-56a664f7a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    if img is None or img.size == 0:\n",
    "        return None\n",
    "\n",
    "    img = cv2.resize(img, (255, 255))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(\"float32\") / 255.0\n",
    "    img = np.transpose(img, (2, 0, 1))  # HWC → CHW\n",
    "    img = torch.from_numpy(img).unsqueeze(0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "271bc3fb-ca85-45ef-9ad8-f991b384c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2a91942-6072-4010-8373-37da5fc0bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    # OpenCV → MediaPipe expects RGB\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS\n",
    "        )\n",
    "        # Get bounding box from landmarks\n",
    "        x_coords = [lm.x for lm in hand_landmarks.landmark]\n",
    "        y_coords = [lm.y for lm in hand_landmarks.landmark]\n",
    "\n",
    "        xmin = int(min(x_coords) * w)\n",
    "        ymin = int(min(y_coords) * h)\n",
    "        xmax = int(max(x_coords) * w)\n",
    "        ymax = int(max(y_coords) * h)\n",
    "\n",
    "        # Add padding\n",
    "        pad = 30\n",
    "        xmin = max(0, xmin - pad)\n",
    "        ymin = max(0, ymin - pad)\n",
    "        xmax = min(w, xmax + pad)\n",
    "        ymax = min(h, ymax + pad)\n",
    "\n",
    "        hand_crop = frame[ymin:ymax, xmin:xmax]\n",
    "\n",
    "        if hand_crop.size != 0:\n",
    "            # Preprocess\n",
    "            hand_rgb = cv2.cvtColor(hand_crop, cv2.COLOR_BGR2RGB)\n",
    "            hand_pil = Image.fromarray(hand_rgb)\n",
    "\n",
    "            input_tensor = transform(hand_pil).unsqueeze(0).to(device)\n",
    "\n",
    "            # Inference\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                probs = torch.softmax(output, dim=1)\n",
    "                conf, pred = torch.max(probs, dim=1)\n",
    "\n",
    "            label = class_names[pred.item()]\n",
    "            confidence = conf.item()\n",
    "\n",
    "            # Draw box + label\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"Class: {label} ({confidence:.2f})\",\n",
    "                (xmin, ymin - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.8,\n",
    "                (0, 255, 0),\n",
    "                2\n",
    "            )\n",
    "\n",
    "    cv2.imshow(\"Hand Gesture Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fbd703f-67fd-470d-8747-7d390c18d2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"
     ]
    }
   ],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d776900-4d34-4fb0-9ee9-180e04862528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier[1].out_features == len(class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
