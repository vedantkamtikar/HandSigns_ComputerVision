{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0811e32c-40c4-47a3-9c55-eab392c7f5a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder, ImageNet\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "import random, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c9f2fd-503e-4a57-bc11-463f54edd43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch can access the GPU.\n",
      "Number of GPUs available: 1\n",
      "Current GPU Name: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch can access the GPU.\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is using the CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ad652a-a711-491c-b253-d2d5c2d32f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(\n",
    "    root=\"archive/train\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = ImageFolder(\n",
    "    root=\"archive/test\",\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1080c3f5-4ee5-430f-af8a-690669587bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n",
      "Number of classes: 24\n"
     ]
    }
   ],
   "source": [
    "class_names = train_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "print(class_names)\n",
    "print(f\"Number of classes: {num_classes}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41749213-dc7e-4b47-974a-3dcb2abfff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27455 7172\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "train_dataset.classes == val_dataset.classes\n",
    "\n",
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c527a5e-a39b-47fd-9508-b1dd265291d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "weights = MobileNet_V2_Weights.DEFAULT\n",
    "model = mobilenet_v2(weights=weights)\n",
    "\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "#freezing the feature extractor\n",
    "\n",
    "model.classifier[1] = nn.Linear(\n",
    "    model.last_channel,\n",
    "    num_classes \n",
    "    #Linear(1280 → 36)\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b05950ad-cd85-443e-987c-6daf3f8578d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.classifier.parameters(),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a4541c8-d3eb-40f6-bcb7-8e43a988e165",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15] - Train Loss: 2.9462, Train Acc: 26.77% - Val Loss: 2.7473, Val Acc: 40.37%\n",
      "Epoch [2/15] - Train Loss: 2.5282, Train Acc: 54.56% - Val Loss: 2.4014, Val Acc: 51.88%\n",
      "Epoch [3/15] - Train Loss: 2.2047, Train Acc: 64.07% - Val Loss: 2.1340, Val Acc: 58.44%\n",
      "Epoch [4/15] - Train Loss: 1.9510, Train Acc: 68.95% - Val Loss: 1.9293, Val Acc: 62.37%\n",
      "Epoch [5/15] - Train Loss: 1.7535, Train Acc: 71.81% - Val Loss: 1.7627, Val Acc: 63.97%\n",
      "Epoch [6/15] - Train Loss: 1.5920, Train Acc: 74.02% - Val Loss: 1.6308, Val Acc: 66.23%\n",
      "Epoch [7/15] - Train Loss: 1.4641, Train Acc: 75.66% - Val Loss: 1.5325, Val Acc: 67.18%\n",
      "Epoch [8/15] - Train Loss: 1.3553, Train Acc: 76.99% - Val Loss: 1.4368, Val Acc: 69.00%\n",
      "Epoch [9/15] - Train Loss: 1.2638, Train Acc: 78.15% - Val Loss: 1.3704, Val Acc: 69.81%\n",
      "Epoch [10/15] - Train Loss: 1.1846, Train Acc: 79.33% - Val Loss: 1.2956, Val Acc: 70.93%\n",
      "Epoch [11/15] - Train Loss: 1.1152, Train Acc: 80.35% - Val Loss: 1.2365, Val Acc: 71.82%\n",
      "Epoch [12/15] - Train Loss: 1.0543, Train Acc: 80.97% - Val Loss: 1.1909, Val Acc: 72.30%\n",
      "Epoch [13/15] - Train Loss: 1.0041, Train Acc: 81.67% - Val Loss: 1.1488, Val Acc: 72.38%\n",
      "Epoch [14/15] - Train Loss: 0.9547, Train Acc: 82.59% - Val Loss: 1.1032, Val Acc: 73.75%\n",
      "Epoch [15/15] - Train Loss: 0.9123, Train Acc: 82.98% - Val Loss: 1.0640, Val Acc: 74.09%\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "        f\"- Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}% \"\n",
    "        f\"- Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "884df1f2-fe1b-491b-b99c-e567ca0b7fce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"mobilent_new.pth\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f1ac31d-9284-43f3-aba6-9f806bac1d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"class_names.json\", \"w\") as f:\n",
    "    json.dump(class_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d0480ad-f84e-4936-b015-555c20543044",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "weights = MobileNet_V2_Weights.DEFAULT\n",
    "model = mobilenet_v2(weights=None)  # no pretrained at inference\n",
    "\n",
    "model.classifier[1] = torch.nn.Linear(\n",
    "    model.last_channel,\n",
    "    len(class_names)\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"mobilent_new.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad2bf30d-dce2-4541-bc38-56a664f7a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    if img is None or img.size == 0:\n",
    "        return None\n",
    "\n",
    "    img = cv2.resize(img, (255, 255))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(\"float32\") / 255.0\n",
    "    img = np.transpose(img, (2, 0, 1))  # HWC → CHW\n",
    "    img = torch.from_numpy(img).unsqueeze(0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "271bc3fb-ca85-45ef-9ad8-f991b384c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2a91942-6072-4010-8373-37da5fc0bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "\n",
    "        # Draw skeleton (visual only)\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS\n",
    "        )\n",
    "\n",
    "        # Landmark bounding box\n",
    "        x_coords = [lm.x for lm in hand_landmarks.landmark]\n",
    "        y_coords = [lm.y for lm in hand_landmarks.landmark]\n",
    "\n",
    "        xmin = int(min(x_coords) * w)\n",
    "        ymin = int(min(y_coords) * h)\n",
    "        xmax = int(max(x_coords) * w)\n",
    "        ymax = int(max(y_coords) * h)\n",
    "\n",
    "        hand_crop = frame[ymin:ymax, xmin:xmax]\n",
    "        \n",
    "        # Make square & scale\n",
    "        box_size = max(xmax - xmin, ymax - ymin)\n",
    "        cx = (xmin + xmax) // 2\n",
    "        cy = (ymin + ymax) // 2\n",
    "\n",
    "        scale = 1.3\n",
    "        half = int(box_size * scale / 2)\n",
    "\n",
    "        xmin = max(0, cx - half)\n",
    "        ymin = max(0, cy - half)\n",
    "        xmax = min(w, cx + half)\n",
    "        ymax = min(h, cy + half)\n",
    "\n",
    "        hand_crop = frame[ymin:ymax, xmin:xmax]\n",
    "\n",
    "        if hand_crop.size != 0:\n",
    "            hand_rgb = cv2.cvtColor(hand_crop, cv2.COLOR_BGR2RGB)\n",
    "            hand_pil = Image.fromarray(hand_rgb)\n",
    "\n",
    "            input_tensor = transform(hand_pil).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                probs = torch.softmax(output, dim=1)\n",
    "                conf, pred = torch.max(probs, dim=1)\n",
    "\n",
    "            label = class_names[pred.item()]\n",
    "            confidence = conf.item()\n",
    "\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"{label} ({confidence:.2f})\",\n",
    "                (xmin, ymin - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.8,\n",
    "                (0, 255, 0),\n",
    "                2\n",
    "            )\n",
    "\n",
    "            cv2.imshow(\"HAND_CROP_DEBUG\", hand_crop)\n",
    "\n",
    "    cv2.imshow(\"Hand Gesture Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fbd703f-67fd-470d-8747-7d390c18d2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d776900-4d34-4fb0-9ee9-180e04862528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier[1].out_features == len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "113c6cb6-db6a-4a90-ab99-d250e4223d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.classes == val_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1df014-2f9a-4214-ab4f-eb3b03a315c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
